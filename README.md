#### lab-spark-kubernetes

å·¥ä½œç›®æ ‡ï¼šé€šè¿‡å°†æ¨¡å‹æœåŠ¡è¿ç§»åˆ°PySparkã€æ•°æ®é›†å¸‚æœåŠ¡è¿ç§»åˆ°Sparkä»¥åŠæ•°æ®æºæœåŠ¡ï¼ŒæŒæ¡ä½¿ç”¨Kubernetesè¿›è¡Œå®¹å™¨ç¼–æ’çš„æŠ€èƒ½ã€‚
å·¥ä½œæ­¥éª¤ï¼š

1. åˆ›å»ºSparkè®¡ç®—åŸºç¡€è®¾æ–½ï¼Œé…ç½®å¤åˆ¶åŠŸèƒ½ï¼š
https://spark.apache.org/docs/latest/running-on-kubernetes.html  https://habr.com/ru/companies/neoflex/articles/511734/ 

2. åœ¨Kubernetesé›†ç¾¤(k8s)ä¸­å¯åŠ¨æ¨¡å‹æœåŠ¡ï¼ˆç¬¬äº”æ¬¡å®éªŒç‰ˆæœ¬ï¼‰ã€‚æ£€æŸ¥å…¶è¿è¡Œæ˜¯å¦æ­£å¸¸ã€‚
3. å¯åŠ¨æ•°æ®æºæœåŠ¡ï¼ˆç¬¬å…­æ¬¡å®éªŒç‰ˆæœ¬ï¼‰ï¼Œå¹¶åœ¨Kubernetesé›†ç¾¤ä¸­éƒ¨ç½²æ¨¡å‹æœåŠ¡çš„æ›´æ–°ã€‚æ£€æŸ¥å…¶è¿è¡Œæ˜¯å¦æ­£å¸¸ã€‚
4. å¯åŠ¨æ•°æ®é›†å¸‚æœåŠ¡ï¼ˆç¬¬ä¸ƒæ¬¡å®éªŒç‰ˆæœ¬ï¼‰ï¼Œå¹¶åœ¨Kubernetesé›†ç¾¤ä¸­éƒ¨ç½²å…¶ä»–ä¸¤ä¸ªæœåŠ¡çš„æ›´æ–°ã€‚æ£€æŸ¥å…¶è¿è¡Œæ˜¯å¦æ­£å¸¸ã€‚
5. ä¼˜åŒ–èµ„æºåˆ©ç”¨ç‡ã€‚å¯ä»¥ç®€åŒ–åŸºç¡€è®¾æ–½ã€‚
6. å…è®¸æ‰“åŒ…ä¸ºHelm Chartï¼Œä½¿ç”¨OpenShiftï¼Œä»¥åŠåœ¨KubernetesæœåŠ¡ä¸­å­˜å‚¨æ•æ„Ÿä¿¡æ¯ï¼ˆsecretsï¼‰ã€‚

___
```
[Source File .csv]
    |
    v
[Data Source Service (Lab6)] # åŠ è½½æºæ–‡ä»¶ä¸Šä¼ åˆ°HDFS
    |
    v
[HDFS Storage]
    |
    v
[Data Mart Service (Lab7)] # åŠ è½½æ•°æ®å¹¶é¢„å¤„ç†ç„¶åæäº¤åˆ°æ•°æ®åº“
    |
    v
[PostgreSQL]
    |
    v
[Model Training Service (Lab5)] # ä»æ•°æ®åº“åŠ è½½æ•°æ®è®­ç»ƒæ¨¡å‹
```
___

#####  Minikube

> ğŸ“– Minikube Doc - https://kubernetes.io/zh-cn/docs/tutorials/hello-minikube/


``` sh
# Step1. Install
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-arm64 
sudo install minikube-darwin-arm64 /usr/local/bin/minikube
minikube version # show the version of minikube

minikube delete
sudo rm -rf /var/lib/minikube
minikube addons enable registry     # æœ¬åœ°é•œåƒä»“åº“
minikube addons enable storage-provisioner  # åŠ¨æ€å­˜å‚¨åˆ†é…

# Step2. Start minikube (driver - docker)
minikube start --driver=docker
minikube status # show the status of server minikube

# Step3. Install kubectl
curl -LO "https://dl.k8s.io/release/$(curl -LSs https://dl.k8s.io/release/stable.txt )/bin/darwin/arm64/kubectl"
chmod +x kubectl
kubectl version --client # show the version of kubectl

# Step4. Install helm
brew install helm
helm version

# Step5. Install operator (ğŸ¤– ç›®å‰æœªè¿›è¡Œä½¿ç”¨)
# https://github.com/apache/spark-kubernetes-operator
helm repo add spark-kubernetes-operator https://apache.github.io/spark-kubernetes-operator
helm repo update
helm install spark-kubernetes-operator spark-kubernetes-operator/spark-kubernetes-operator
helm uninstall spark-kubernetes-operator

kubectl get crd | grep spark
```

##### Set HDFS ï¼ˆLab æä¾›æ•°æ®æºçš„éƒ¨åˆ†ï¼‰


``` bash
kubectl logs <datanode_name> --previous
kubectl logs $(kubectl get pods | grep datamart | awk '{print $1}') --previous 
kubectl logs datanode-664bcc4c76-vlk4r # æŸ¥çœ‹æ—¥å¿—
kubectl exec -it $(kubectl get pods | grep namenode | awk '{print $1}') -- bash # è¿›å…¥namenodeçš„shell
hdfs dfs -mkdir -p /data/{input,output,backup} # åˆ›å»ºæ•°æ®æ¹–çš„ç›®å½•
hdfs dfs -ls /data/ # æŸ¥çœ‹éªŒè¯æ˜¯å¦å­˜åœ¨åˆ›å»ºçš„æ–‡ä»¶
kubectl cp ./data/en.openfoodfacts.org.products.csv.gz <namenode_name>:/tmp/ # æœ¬åœ°æ–‡ä»¶æ‹·è´åˆ°å®¹å™¨å†…
hdfs dfs -put /tmp/en.openfoodfacts.org.products.csv.gz /data/input/

minikube service namenode --url
```

##### Data mart (HDFS -> Data_mart -> Database)

``` bash
# Step1. Set the secret info & config. 
kubectl create secret generic pg-secrets --from-env-file=secret.env - kubectl delete secret pg-secrets
kubectl create configmap app-config --from-env-file=config.env

# Step2. Apply -f
kubectl apply -f ./database_init

# Step3. Check the database.
kubectl exec -it $(kubectl get pod -l app=postgres -o jsonpath='{.items[0].metadata.name}') -- psql -U postgres -d datamartdb -c 'SELECT * FROM processed_data;'
# jdbc:postgresql://postgres:5432/datamartdb

# Step4. Create Jar & Upload.
sbt clean assembly
kubectl cp ./target/scala-2.12/datamart-assembly-0.1.jar <namenode_name>:/tmp/
kubectl exec -it $(kubectl get pods | grep namenode | awk '{print $1}') -- bash 
hdfs dfs -put /tmp/datamart-assembly-0.1.jar /spark-uploads

# Step5. Upload to the Minikube.
eval $(minikube docker-env) - eval $(minikube docker-env -u)
docker build -t datamart:latest .

kubectl exec -it $(kubectl get pods | grep namenode | awk '{print $1}') -- bash 
hdfs dfs -ls hdfs://namenode:9000/ # æµ‹è¯•
hdfs dfs -mkdir -p /spark-uploads
hdfs dfs -chmod 777 /spark-uploads
kubectl apply -f ./job-datamart.yaml - kubectl delete job datamart-job
```

##### @TODO 

```
- ä¿®æ”¹secretè®¾ç½®çš„æ–¹å¼
- æäº¤ä¸æˆåŠŸï¼Œè€ƒè™‘æ­£ç¡®é…ç½®æ‰€æœ‰ä¸œè¥¿


Helm Chart	
æŠŠé…ç½®å‚æ•°åŒ–ï¼Œä¾¿äºä¸åŒç¯å¢ƒå¤ç”¨
ConfigMap	
å­˜æ”¾éæ•æ„Ÿé…ç½®å¦‚æ•°æ®åº“åœ°å€ã€HDFS åœ°å€
Secret	
å­˜æ”¾ç”¨æˆ·åå¯†ç ç­‰æ•æ„Ÿä¿¡æ¯
```