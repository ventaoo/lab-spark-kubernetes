#### lab-spark-kubernetes

å·¥ä½œç›®æ ‡ï¼šé€šè¿‡å°†æ¨¡å‹æœåŠ¡è¿ç§»åˆ°PySparkã€æ•°æ®é›†å¸‚æœåŠ¡è¿ç§»åˆ°Sparkä»¥åŠæ•°æ®æºæœåŠ¡ï¼ŒæŒæ¡ä½¿ç”¨Kubernetesè¿›è¡Œå®¹å™¨ç¼–æ’çš„æŠ€èƒ½ã€‚
å·¥ä½œæ­¥éª¤ï¼š

1. åˆ›å»ºSparkè®¡ç®—åŸºç¡€è®¾æ–½ï¼Œé…ç½®å¤åˆ¶åŠŸèƒ½ï¼š
https://spark.apache.org/docs/latest/running-on-kubernetes.html  https://habr.com/ru/companies/neoflex/articles/511734/ 

2. åœ¨Kubernetesé›†ç¾¤(k8s)ä¸­å¯åŠ¨æ¨¡å‹æœåŠ¡ï¼ˆç¬¬äº”æ¬¡å®éªŒç‰ˆæœ¬ï¼‰ã€‚æ£€æŸ¥å…¶è¿è¡Œæ˜¯å¦æ­£å¸¸ã€‚
3. å¯åŠ¨æ•°æ®æºæœåŠ¡ï¼ˆç¬¬å…­æ¬¡å®éªŒç‰ˆæœ¬ï¼‰ï¼Œå¹¶åœ¨Kubernetesé›†ç¾¤ä¸­éƒ¨ç½²æ¨¡å‹æœåŠ¡çš„æ›´æ–°ã€‚æ£€æŸ¥å…¶è¿è¡Œæ˜¯å¦æ­£å¸¸ã€‚
4. å¯åŠ¨æ•°æ®é›†å¸‚æœåŠ¡ï¼ˆç¬¬ä¸ƒæ¬¡å®éªŒç‰ˆæœ¬ï¼‰ï¼Œå¹¶åœ¨Kubernetesé›†ç¾¤ä¸­éƒ¨ç½²å…¶ä»–ä¸¤ä¸ªæœåŠ¡çš„æ›´æ–°ã€‚æ£€æŸ¥å…¶è¿è¡Œæ˜¯å¦æ­£å¸¸ã€‚
5. ä¼˜åŒ–èµ„æºåˆ©ç”¨ç‡ã€‚å¯ä»¥ç®€åŒ–åŸºç¡€è®¾æ–½ã€‚
6. å…è®¸æ‰“åŒ…ä¸ºHelm Chartï¼Œä½¿ç”¨OpenShiftï¼Œä»¥åŠåœ¨KubernetesæœåŠ¡ä¸­å­˜å‚¨æ•æ„Ÿä¿¡æ¯ï¼ˆsecretsï¼‰ã€‚

___
```
[Source File .csv]
    |
    v
[Data Source Service (Lab6)] # åŠ è½½æºæ–‡ä»¶ä¸Šä¼ åˆ°HDFS
    |
    v
[HDFS Storage]
    |
    v
[Data Mart Service (Lab7)] # åŠ è½½æ•°æ®å¹¶é¢„å¤„ç†ç„¶åæäº¤åˆ°æ•°æ®åº“
    |
    v
[PostgreSQL]
    |
    v
[Model Training Service (Lab5)] # ä»æ•°æ®åº“åŠ è½½æ•°æ®è®­ç»ƒæ¨¡å‹
```
___

#####  Minikube

> ğŸ“– Minikube Doc - https://kubernetes.io/zh-cn/docs/tutorials/hello-minikube/


``` sh
# Step1. Install
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-arm64 
sudo install minikube-darwin-arm64 /usr/local/bin/minikube
minikube version # show the version of minikube

minikube delete
sudo rm -rf /var/lib/minikube
minikube addons enable registry     # æœ¬åœ°é•œåƒä»“åº“
minikube addons enable storage-provisioner  # åŠ¨æ€å­˜å‚¨åˆ†é…

# Step2. Start minikube (driver - docker)
minikube start --driver=docker 
minikube status # show the status of server minikube

# Step3. Install kubectl
curl -LO "https://dl.k8s.io/release/$(curl -LSs https://dl.k8s.io/release/stable.txt )/bin/darwin/arm64/kubectl"
chmod +x kubectl
kubectl version --client # show the version of kubectl

# Step4. Install helm
brew install helm
helm version
```

##### HDFS helm

- Step1. Install
``` bash
helm install hdfs-cluster ./hdfs-helm
```

- Step2. Upload the file (`source_data`) to HDFS
``` bash
kubectl exec -it $(kubectl get pods | grep namenode | awk '{print $1}') -- bash
hdfs dfs -mkdir -p /data/{input,output,backup}

kubectl cp ./files/en.openfoodfacts.org.products.csv.gz $(kubectl get pods | grep namenode | awk '{print $1}'):/tmp/ 

kubectl exec -it $(kubectl get pods | grep namenode | awk '{print $1}') -- bash
hdfs dfs -put /tmp/en.openfoodfacts.org.products.csv.gz /data/input/
```

- Step3. Create Jar & Upload
```bash
sbt clean assembly

kubectl exec -it $(kubectl get pods | grep namenode | awk '{print $1}') -- bash 
hdfs dfs -mkdir -p /spark-uploads
hdfs dfs -chmod 777 /spark-uploads

kubectl cp ./target/scala-2.12/datamart-assembly-0.1.jar $(kubectl get pods | grep namenode | awk '{print $1}'):/tmp/
kubectl cp ./jar/target/scala-2.12/postgresql-42.6.0.jar $(kubectl get pods | grep namenode | awk '{print $1}'):/tmp/ 
kubectl exec -it $(kubectl get pods | grep namenode | awk '{print $1}') -- bash 
hdfs dfs -put /tmp/*.jar /spark-uploads
```

- Step4. Check file in the web
``` bash
minikube service hdfs-cluster-namenode --url
```

##### Datamart helm (HDFS -> Data_mart -> Database)

![Spark in k8s](https://spark.apache.org/docs/latest/img/k8s-cluster-mode.png)

- Step1. Init database
``` bash
helm install postgresql ./postgresql-helm --set postgresql.password=postgres

kubectl exec -it $(kubectl get pods | grep postgres | awk '{print $1}') -- psql -U postgres -d datamartdb -c 'SELECT * FROM processed_data LIMIT 5;'
```

- Step2. Image to Minikube
``` bash
eval $(minikube docker-env) - eval $(minikube docker-env -u)
docker build -t datamart:latest .
```

- Step3. Install
``` bash
helm install datamart ./spark-job-chart \                                               
  --set hdfs.namenode="hdfs-cluster-namenode" \
  --set db.host="postgresql-postgres" \
  --set db.password=postgres
```

##### Database -> model training

``` bash
docker build -t model-training:latest .

# å®‰è£…Chartï¼ˆå‡è®¾å·²å­˜åœ¨datamart-db-secretï¼‰
helm install model-training ./model-training-chart \ 
  --set db.password=postgres
```


``` bash
kubectl get pods
model-training-job-kvd4c                    0/1     Completed   0               4h22m


kubectl describe pv pvc-a6186cf4-b0b1-4605-9f99-e6367092bf75

Name:            pvc-a6186cf4-b0b1-4605-9f99-e6367092bf75
Labels:          <none>
Annotations:     hostPathProvisionerIdentity: 0b082bb0-2b8b-45ba-a1ef-dd1715e75693
                 pv.kubernetes.io/provisioned-by: k8s.io/minikube-hostpath
Finalizers:      [kubernetes.io/pv-protection]
StorageClass:    standard
Status:          Bound
Claim:           default/training-data-pvc
Reclaim Policy:  Delete
Access Modes:    RWO
VolumeMode:      Filesystem
Capacity:        1Gi
Node Affinity:   <none>
Message:         
Source:
    Type:          HostPath (bare host directory volume)
    Path:          /tmp/hostpath-provisioner/default/training-data-pvc
    HostPathType:  
Events:            <none>

minikube ssh

docker@minikube:~$ cd /tmp/hostpath-provisioner/default/training-data-pvc

ls
app.log  model  spark-events
```